{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Simples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "\n",
    "* <a href=\"\">Introdução</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"#id1\">1) Introdução</h2>\n",
    "\n",
    "A regressão linear simples modela o relacionamento entre a magnitude de uma variável e aquela de uma segunda - por exemplo, conforme X aumenta, Y também aumenta. Ou conforme X aumenta, Y diminui. A correlação é outro jeito de medir como duas variáveis estão relacionadas. A diferença é que enquanto a correlação mede a força de uma associação entre duas variáveis, a regressão quantifica a natureza do relacionamento.\n",
    "\n",
    "É um processo de Machine Learning supervisionado. É semelhante à classificação, mas, em vez de prever um rótulo, tentamos prever um valor contínuo. Se você estiver tentando prever um número, utilize a regressão.\n",
    "\n",
    "* **Variável independente (x) -> Variável dependente (y)**<br/>\n",
    "Análise de regressão é uma metodologia estatística que utiliza a relação entre duas ou mais variáveis quantitativas de tal forma que uma variável possa ser predita a partir de outra.\n",
    "\n",
    "<h3 id=\"#id1_1\">1.1) Termos-Chave</h3>\n",
    "\n",
    "* **Resposta**: A variável que estamos tentando prever. *Sinônimos*: Variável dependente, variável Y, alvo, resultado.\n",
    "* **Variável independente**: A variável usada para prever a resposta. *Sinônimos*: Variável X, característica, atributo.\n",
    "* **Registro**: O vetor de valores preditor e de resultado para um indivíduo ou caso específico. *Sinônimos*: linha, caso, exemplo.\n",
    "* **Intercepto**: O interceptor da linha de regressão - ou seja, o valor previsto quando X = 0. *Sinônimos*: b0, ß0.\n",
    "* **Coeficiente de Regressão**: O declive da linha de regressão s. *Sinônimos*: declive, b1, ß1, estimativas de parâmetro, pesos.\n",
    "* **Valores ajustados**: As estimativas Ŷi obtidas da linha de regressão. *Sinônimos*: Valores previstos.\n",
    "* **Resíduo**: A diferença entre os valores observados e os valores ajustados. *Sinônimos*: erros.\n",
    "* **Mínimos Quadrados**: O método de ajustar uma regressão pela minimização da soma dos quadrados dos resíduos. *Sinônimos*: mínimos quadrados ordinários.\n",
    "\n",
    "---\n",
    "\n",
    "<h2 id=\"#id2\">2) A Equação de Regressão</h2>\n",
    "\n",
    "A regressão linear simples estima exatamente o quanto Y mudará quando X mudar em uma certa quantidade. Com o coeficiente de correlação, as variáveis X e Y são intercambiáveis. Com a regressão, estamos tentando prever a variável Y a partir de X usando um relacionamento linear (ou seja, uma linha):\n",
    "\n",
    "<img src=\"assets/00_linearRegression.jpg\" alt=\"Fórmula da regressão linear\" width=\"450\">\n",
    "\n",
    "Quando resolvida, temos um intercepto e um coeficiente. O intercepto nos dá um valor de base para uma predição, modificado pela soma do produto entre o coeficiente e o dado de entrada. Esse formato pode ser generalizado para dimensões maiores. Nesse caso, cada atributo terá um coeficiente. Quando maior o valor absoluto do coeficiente, mais impacto terá o atributo no alvo.\n",
    "\n",
    "O modelo pressupõe que a predição é uma combinação linear dos dados de entrada. Para alguns conjuntos de dados, isso não será suficientemente flexível. Mais complexidade poderá ser acrescentada por meio da transformação dos atributos (o transformador `preprocessing.PolynomialFeatures` do sklearn é capaz de criar combinações polinomiais dos atributos). Se isso resultar em overfitting, regressões ridge e lasso poderão ser usadas para regularizar o estimador. \n",
    "\n",
    "Esse modelo também é suscetível a **heterocedasticidade**. A heterocedasticidade é a ideia de que, à medida que os valores de entrada mudam, o erro da predição (ou resíduos) geralmente muda também.\n",
    "\n",
    "Outra questão que deve ser considerada é a **multicolinearidade**. Se as colunas tiverem um alto nível de correlação, será mais difícil interpretar os coeficientes. Em geral, isso não causará impactos no modelo, mas apenas no significado dos coeficientes.\n",
    "\n",
    "<h3 id=\"#id2_1\">2.1) Propriedades do modelo</h3>\n",
    "\n",
    "Um modelo de regressão linear tem as seguintes propriedades:\n",
    "* **Eficiência na execução**: Use n_jobs para melhorar o desempenho.\n",
    "* **Pré-processamento dos dados**: Padronize os dados antes de fazer o treinamento do modelo.\n",
    "* **Para evitar overfitting**: Você pode simplificar o modelo não usando ou adicionando atributos polinomiais.\n",
    "* **Interpretação dos resultados**: É possível interpretar os resultados como pesos para contribuição dos atributos, mas supõe-se que os atributos tenham uma distribuição normal e sejam independentes. Você pode remover atributos colineares para facilitar a interpretação. R² informará até que ponto a variância total do resultado é explicada pelo modelo.\n",
    "\n",
    "Parâmetros da instância:\n",
    "* **n_jobs**: Número de CPUs a ser usadas. -1 se todas.\n",
    "\n",
    "Atributos após a adequação:\n",
    "* **coef_**: Coeficientes da regressão linear.\n",
    "* **intercept_**: Intercepto do modelo linear.\n",
    "\n",
    "<h3 id=\"#id2_2\">2.2) Exemplo básico do algoritmo</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "df = pd.read_csv('../data/bostonhousing.csv')\n",
    "bos_X = df.drop(columns=['MEDV'])\n",
    "bos_y = df['MEDV']\n",
    "\n",
    "for col in bos_X:\n",
    "    bos_X[col].fillna(bos_X[col].median(), inplace=True)\n",
    "\n",
    "\n",
    "bos_sX = preprocessing.StandardScaler().fit_transform(bos_X)\n",
    "bos_sX_train, bos_sX_test, bos_sy_train, bos_sy_test = model_selection.train_test_split(bos_sX, bos_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707643863232741\n",
      "[-1.06748826  0.69833808 -0.0531295   0.93616181 -1.60647927  3.07697382\n",
      " -0.66808491 -2.96024937  1.65782613 -1.19340249 -1.92582778  1.05147265\n",
      " -3.37215831]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(bos_sX_train, bos_sy_train)\n",
    "print(lr.score(bos_sX_test, bos_sy_test))\n",
    "print(lr.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
