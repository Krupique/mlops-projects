{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Simples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "\n",
    "* <a href=\"\">Introdução</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"#id1\">1) Introdução</h2>\n",
    "\n",
    "A regressão linear simples modela o relacionamento entre a magnitude de uma variável e aquela de uma segunda - por exemplo, conforme X aumenta, Y também aumenta. Ou conforme X aumenta, Y diminui. A correlação é outro jeito de medir como duas variáveis estão relacionadas. A diferença é que enquanto a correlação mede a força de uma associação entre duas variáveis, a regressão quantifica a natureza do relacionamento.\n",
    "\n",
    "É um processo de Machine Learning supervisionado. É semelhante à classificação, mas, em vez de prever um rótulo, tentamos prever um valor contínuo. Se você estiver tentando prever um número, utilize a regressão.\n",
    "\n",
    "* **Variável independente (x) -> Variável dependente (y)**<br/>\n",
    "Análise de regressão é uma metodologia estatística que utiliza a relação entre duas ou mais variáveis quantitativas de tal forma que uma variável possa ser predita a partir de outra.\n",
    "\n",
    "<h3 id=\"#id1_1\">1.1) Termos-Chave</h3>\n",
    "\n",
    "* **Resposta**: A variável que estamos tentando prever. *Sinônimos*: Variável dependente, variável Y, alvo, resultado.\n",
    "* **Variável independente**: A variável usada para prever a resposta. *Sinônimos*: Variável X, característica, atributo.\n",
    "* **Registro**: O vetor de valores preditor e de resultado para um indivíduo ou caso específico. *Sinônimos*: linha, caso, exemplo.\n",
    "* **Intercepto**: O interceptor da linha de regressão - ou seja, o valor previsto quando X = 0. *Sinônimos*: b0, ß0.\n",
    "* **Coeficiente de Regressão**: O declive da linha de regressão s. *Sinônimos*: declive, b1, ß1, estimativas de parâmetro, pesos.\n",
    "* **Valores ajustados**: As estimativas Ŷi obtidas da linha de regressão. *Sinônimos*: Valores previstos.\n",
    "* **Resíduo**: A diferença entre os valores observados e os valores ajustados. *Sinônimos*: erros.\n",
    "* **Mínimos Quadrados**: O método de ajustar uma regressão pela minimização da soma dos quadrados dos resíduos. *Sinônimos*: mínimos quadrados ordinários.\n",
    "\n",
    "---\n",
    "\n",
    "<h2 id=\"#id2\">2) A Equação de Regressão</h2>\n",
    "\n",
    "A regressão linear simples estima exatamente o quanto Y mudará quando X mudar em uma certa quantidade. Com o coeficiente de correlação, as variáveis X e Y são intercambiáveis. Com a regressão, estamos tentando prever a variável Y a partir de X usando um relacionamento linear (ou seja, uma linha):\n",
    "\n",
    "<img src=\"assets/00_linearRegression.jpg\" alt=\"Fórmula da regressão linear\" width=\"450\">\n",
    "\n",
    "Quando resolvida, temos um intercepto e um coeficiente. O intercepto nos dá um valor de base para uma predição, modificado pela soma do produto entre o coeficiente e o dado de entrada. Esse formato pode ser generalizado para dimensões maiores. Nesse caso, cada atributo terá um coeficiente. Quando maior o valor absoluto do coeficiente, mais impacto terá o atributo no alvo.\n",
    "\n",
    "O modelo pressupõe que a predição é uma combinação linear dos dados de entrada. Para alguns conjuntos de dados, isso não será suficientemente flexível. Mais complexidade poderá ser acrescentada por meio da transformação dos atributos (o transformador `preprocessing.PolynomialFeatures` do sklearn é capaz de criar combinações polinomiais dos atributos). Se isso resultar em overfitting, regressões ridge e lasso poderão ser usadas para regularizar o estimador. \n",
    "\n",
    "Esse modelo também é suscetível a **heterocedasticidade**. A heterocedasticidade é a ideia de que, à medida que os valores de entrada mudam, o erro da predição (ou resíduos) geralmente muda também.\n",
    "\n",
    "Outra questão que deve ser considerada é a **multicolinearidade**. Se as colunas tiverem um alto nível de correlação, será mais difícil interpretar os coeficientes. Em geral, isso não causará impactos no modelo, mas apenas no significado dos coeficientes.\n",
    "\n",
    "<h3 id=\"#id2_1\">2.1) Propriedades do modelo</h3>\n",
    "\n",
    "Um modelo de regressão linear tem as seguintes propriedades:\n",
    "* **Eficiência na execução**: Use n_jobs para melhorar o desempenho.\n",
    "* **Pré-processamento dos dados**: Padronize os dados antes de fazer o treinamento do modelo.\n",
    "* **Para evitar overfitting**: Você pode simplificar o modelo não usando ou adicionando atributos polinomiais.\n",
    "* **Interpretação dos resultados**: É possível interpretar os resultados como pesos para contribuição dos atributos, mas supõe-se que os atributos tenham uma distribuição normal e sejam independentes. Você pode remover atributos colineares para facilitar a interpretação. R² informará até que ponto a variância total do resultado é explicada pelo modelo.\n",
    "\n",
    "Parâmetros da instância:\n",
    "* **n_jobs**: Número de CPUs a ser usadas. -1 se todas.\n",
    "\n",
    "Atributos após a adequação:\n",
    "* **coef_**: Coeficientes da regressão linear.\n",
    "* **intercept_**: Intercepto do modelo linear.\n",
    "\n",
    "O valor do **.intercept_** é o valor médio esperado. Podemos ver como o fato de escalar os dados afeta os coeficientes. O sinal dos coeficientes explica a direção da relação entre o atributo e o alvo (target). Um sinal positivo informa que, à medida que o atributo aumenta, o rótulo (label) aumenta. Um sinal negativo mmostra que, à medida que o atributo aumenta, o rótulo diminui. Quanto maior o valor absoluto do coeficiente, mais impacto ele causará.\n",
    "\n",
    "<h3 id=\"#id2_2\">2.2) Exemplo básico do algoritmo</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE\n",
       "0  65.2\n",
       "1  78.9\n",
       "2  61.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "df = pd.read_csv('../data/bostonhousing.csv')\n",
    "bos_X = df.drop(columns=['MEDV'])\n",
    "bos_X = bos_X[['AGE']]\n",
    "bos_y = df['MEDV']\n",
    "\n",
    "display(bos_X.head(3))\n",
    "display(bos_y.head(3))\n",
    "\n",
    "for col in bos_X:\n",
    "    bos_X[col].fillna(bos_X[col].median(), inplace=True)\n",
    "\n",
    "bos_sX = preprocessing.StandardScaler().fit_transform(bos_X)\n",
    "bos_sX_train, bos_sX_test, bos_sy_train, bos_sy_test = model_selection.train_test_split(bos_sX, bos_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15388228824187467\n",
      "Coef: [-3.40549831]\n",
      "Intercept: 23.040506838183067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(bos_sX_train, bos_sy_train)\n",
    "print(lr.score(bos_sX_test, bos_sy_test))\n",
    "print(f\"Coef: {lr.coef_}\")\n",
    "print(f\"Intercept: {lr.intercept_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns conceitos importantes na análise de regressão são os valores *ajustados* e *resíduos*. Em geral, os dados não ficam exatamente em linha, então a equação de regressão deveria incluir um termo explícito de erro ei.\n",
    "\n",
    "Os valores ajustados, também chamados de valores *previstos*, são geralmente denotados por **Ŷ**. As notações de *b0(chapéu)* e *b1(chapéu)* indicam que os coeficientes são os estimados versus os conhecidos.\n",
    "\n",
    "Notação Chapéu: Estimativas versus valores conhecidos. A notação \"chapéu\" é usada para diferenciar as estimativas dos valores conhecidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"#id2_3\">2.3) Mínimos Quadrados</h3>\n",
    "\n",
    "Como o modelo é ajustado aos dados? Quando existe um relacionamento claro, pode-se imaginar ajustar a linha à mão. Na prática, a linha de regressão é a estiamtiva que minimiza a soma dos valores quadrados do resíduo, também chamados de *residual sum of squares* ou RSS:\n",
    "\n",
    "<img src=\"assets/01_residual_sum_squares.png\" alt=\"RSS\" width=\"400\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estimativas b0 (chapéu) e b1 (chapéu) são os valores que minimizam a RSS. O método de minizar a soma dos resíduos quadrados é chamado de regressão de mínimos quadrados, ou regressão de mínimos quadrados ordinários (Ordinary Least Squares - OLS). \n",
    "\n",
    "Os quadrados mínimos, como média, são sensíveis aos outliers, porém isso costuma ser um problema significativo apenas em problemas pequenos ou de tamanho moderado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"#id2_4\">2.4) Previsão versus Explicação (Profiling)</h3>\n",
    "\n",
    "Historicamente, um dos primeiros usos da regressão era desvendar um suposto relacionamento linear entre as variáveis preditoras e uma variável resultante. O objetivo tem sido entender um relacionamento e explicá-lo usandos os dados para os quais a regressão foi ajustada. \n",
    "\n",
    "Nesse caso, o objetivo principal está no declive estimado da equação de regressão, b(chapéu). Ou seja, de modo geral, o objetivo não é prever casos individuais, mas, sim, entender o relacionamento geral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
